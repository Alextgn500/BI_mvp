"""
–†–∞–±–æ—Ç–∞ —Å ML-–º–æ–¥–µ–ª—è–º–∏: –æ–±—É—á–µ–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.
"""

import pickle
from collections.abc import Sequence
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from app.logger import logger

# ==================== –°–¢–ê–†–´–ï –§–£–ù–ö–¶–ò–ò (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å) ====================

# ‚úÖ –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ model_store –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
# app/services/ml_models.py -> app/services -> app -> fastapi_ml
BASE_DIR = Path(__file__).parent.parent.parent  # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è 3 —É—Ä–æ–≤–Ω—è
MODEL_DIR = BASE_DIR / "model_store"
MODEL_PATH = MODEL_DIR / "model.pkl"

# ‚úÖ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–ü–û–°–õ–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)
print(f"üîç –¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª: {__file__}")
print(f"üîç BASE_DIR: {BASE_DIR}")
print(f"üîç MODEL_DIR: {MODEL_DIR}")
print(f"üîç MODEL_PATH: {MODEL_PATH}")
print(f"üîç –ú–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {MODEL_PATH.exists()}")


def train_dummy_model(_epochs: int = 1) -> Any:
    """
    –¢—Ä–µ–Ω–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç—É—é –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    feature_matrix = np.arange(10).reshape(-1, 1)
    y = 2.0 * feature_matrix.ravel() + 1.0
    model = LinearRegression()
    model.fit(feature_matrix, y)
    return model


def save_model(model: Any) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ñ–∞–π–ª–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (pickle).
    """
    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(model, f)
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")


def load_model() -> Any:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
    """
    if not MODEL_PATH.exists():
        logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é")
        model = train_dummy_model()
        save_model(model)
        return model

    with open(MODEL_PATH, "rb") as f:
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {MODEL_PATH}")
        return pickle.load(f)


def predict_from_model(model: Any, features: Sequence[float]) -> float:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–∑ –º–æ–¥–µ–ª–∏.
    """
    if not features:
        raise ValueError("features is empty")

    feature_matrix = np.array(features).reshape(1, -1)
    pred = model.predict(feature_matrix)

    try:
        return float(pred[0])
    except Exception as e:
        raise ValueError(f"Unexpected prediction output: {pred!r}") from e


# ==================== –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨ –î–õ–Ø –ü–†–û–ì–ù–û–ó–ê –ü–†–û–î–ê–ñ ====================


class SalesForecastModel:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂ –Ω–∞ –æ—Å–Ω–æ–≤–µ Random Forest.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü, –¥–µ–Ω—å –º–µ—Å—è—Ü–∞)
    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–≥–∞–∑–∏–Ω–∞—Ö
    - –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–Ω–¥—ã
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏."""
        self.model = None
        self.shop_encoder = LabelEncoder()

        # ‚úÖ –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ model_store
        base_dir = Path(__file__).parent.parent.parent
        model_store = base_dir / "model_store"

        self.model_path = model_store / "sales_rf_model.joblib"
        self.encoder_path = model_store / "shop_encoder.joblib"
        self.metadata_path = model_store / "model_metadata.joblib"

        # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_columns = [
            "day_of_week",
            "day_of_month",
            "month",
            "shop_encoded",
            "days_since_start",
        ]

        self.start_date = None
        self.train_date = None

    def prepare_features(
        self, df: pd.DataFrame, fit_encoder: bool = False
    ) -> pd.DataFrame:
        """
            –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏.

            Args:
        df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [date, shop, amount]
                fit_encoder: –û–±—É—á–∏—Ç—å –ª–∏ —ç–Ω–∫–æ–¥–µ—Ä –º–∞–≥–∞–∑–∏–Ω–æ–≤ (True –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)

            Returns:
                DataFrame —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        df = df.copy()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º date –≤ datetime
        if not pd.api.types.is_datetime64_any_dtype(df["date"]):
            df["date"] = pd.to_datetime(df["date"])

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df["day_of_week"] = df["date"].dt.dayofweek
        df["day_of_month"] = df["date"].dt.day
        df["month"] = df["date"].dt.month

        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–æ–≤
        if fit_encoder:
            df["shop_encoded"] = self.shop_encoder.fit_transform(df["shop"])
        else:
            # –ü—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä
            df["shop_encoded"] = self.shop_encoder.transform(df["shop"])

        # –î–Ω–∏ —Å –Ω–∞—á–∞–ª–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        if self.start_date is None:
            self.start_date = df["date"].min()

        df["days_since_start"] = (df["date"] - self.start_date).dt.days

        return df

    def train(
        self,
        df: pd.DataFrame,
        n_estimators: int = 100,
        max_depth: int = 10,
        test_size: float = 0.2,
        random_state: int = 42,
    ) -> dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest.

        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [date, shop, amount]
            n_estimators: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
            max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df_features = self.prepare_features(df, fit_encoder=True)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ X –∏ y
        X = df_features[self.feature_columns]
        y = df_features["amount"]

        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            n_jobs=-1,
        )

        self.model.fit(X_train, y_train)

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = dict(
            zip(self.feature_columns, self.model.feature_importances_, strict=False)
        )

        self.train_date = datetime.now()

        metrics = {
            "train_r2": float(train_score),
            "test_r2": float(test_score),
            "feature_importance": feature_importance,
            "n_train_samples": len(X_train),
            "n_test_samples": len(X_test),
            "train_date": self.train_date.isoformat(),
        }

        logger.info(
            f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ | R¬≤ train: {train_score:.3f} | R¬≤ test: {test_score:.3f}"
        )

        return metrics

    def predict(
        self, shop: str, target_date: date | str, days_ahead: int = 7
    ) -> list[dict[str, Any]]:
        """
        –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ –Ω–∞ –±—É–¥—É—â–µ–µ.

        Args:
            shop: –ù–∞–∑–≤–∞–Ω–∏–µ –º–∞–≥–∞–∑–∏–Ω–∞
            target_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
            days_ahead: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ [{date, shop, predicted_amount}, ...]
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ train() –∏–ª–∏ load_model()")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ date
        if isinstance(target_date, str):
            target_date = datetime.strptime(target_date, "%Y-%m-%d").date()

        # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        dates = [target_date + timedelta(days=i) for i in range(days_ahead)]
        df_predict = pd.DataFrame({"date": dates, "shop": shop})

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_features = self.prepare_features(df_predict, fit_encoder=False)
        X = df_features[self.feature_columns]

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = self.model.predict(X)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results = []
        for i, pred_date in enumerate(dates):
            results.append(
                {
                    "date": pred_date.isoformat(),
                    "shop": shop,
                    "predicted_amount": float(predictions[i]),
                }
            )

        return results

    def save_model(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        if self.model is None:
            raise ValueError("–ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.model_path.parent.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        joblib.dump(self.model, self.model_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–Ω–∫–æ–¥–µ—Ä
        joblib.dump(self.shop_encoder, self.encoder_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "start_date": self.start_date,
            "train_date": self.train_date,
            "feature_columns": self.feature_columns,
        }
        joblib.dump(metadata, self.metadata_path)

        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")

    def load_model(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.

        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, False –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        """
        if not self.model_path.exists():
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
            return False

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = joblib.load(self.model_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä
        self.shop_encoder = joblib.load(self.encoder_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = joblib.load(self.metadata_path)
        self.start_date = metadata["start_date"]
        self.train_date = metadata["train_date"]
        self.feature_columns = metadata["feature_columns"]

        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_path}")
        return True
